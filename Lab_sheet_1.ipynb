{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6vZfOHWoYPj"
      },
      "source": [
        "# **Lab Sheet: Text Mining Preprocessing with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNCf5ILGod9J"
      },
      "source": [
        "**Objectives**\n",
        "\n",
        "1.   Implement basic text preprocessing techniques.\n",
        "2.Clean and prepare text data for further analysis.\n",
        "2.   Use Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models to vectorize text data.\n",
        "3.   Understand and apply common text mining techniques.\n",
        "5.Use libraries such as nltk and re to perform common preprocessing tasks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlsL5fRqpJtn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mtG5nSpfpy"
      },
      "source": [
        "**Prerequisites**\n",
        "\n",
        "*   Basic knowledge of Python programming.\n",
        "*   Understanding of text processing concepts.\n",
        "* Python installed on your machine.\n",
        "* Internet connection to download necessary libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6von8yGwpyOu"
      },
      "source": [
        "**Instructions**\n",
        "\n",
        "**1. Set Up Your Environment**\n",
        "1. Install Python: Make sure you have Python 3.x installed. You can download it from python.org.\n",
        "\n",
        "2. Create and Activate a Virtual Environment (optional but recommended):\n",
        "\n",
        "* Create a virtual environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZImJVyuPp_An"
      },
      "outputs": [],
      "source": [
        "python -m venv myenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3sc6wXOqCNk"
      },
      "source": [
        "* Activate the virtual environment\n",
        "  * On Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZodQjtpLqMdo"
      },
      "outputs": [],
      "source": [
        "myenv\\Scripts\\activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4zs7_FIqPL8"
      },
      "source": [
        "   * On macOS/Linux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgroKgZfoR_O"
      },
      "outputs": [],
      "source": [
        "source myenv/bin/activate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6pyMCvsqbWs"
      },
      "source": [
        "3. Install Required Libraries:\n",
        "* Install 'nltk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZEqwH-xqiAk",
        "outputId": "e8e877fd-c7a2-4352-c68f-dc25ddb2e2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqU2lIubql-w"
      },
      "source": [
        "**2. Download NLTK Resources**\n",
        "\n",
        "You need to download specific resources from 'nltk' to use stopwords and tokenizers. Run the following code to download these resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9REAMWKqrNo",
        "outputId": "9921c7a2-d350-4090-cb9f-527a52ce6408"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGblDLqBqthU"
      },
      "source": [
        "**3. Implement the Preprocessing Code**\n",
        "\n",
        "Create a Python script or Jupyter Notebook with the following code to perform text preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94-YPJfyqzxQ",
        "outputId": "c8c258ef-db7b-4a98-8d3e-0d3d844105d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'world', 'exampl', 'text', 'preprocess', 'python', 'let', 'clean', 'text']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer and stopwords\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and perform stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello world! This is an example of text preprocessing in Python. Let's clean this text.\"\n",
        "\n",
        "# Preprocess the sample text\n",
        "preprocessed_tokens = preprocess_text(text)\n",
        "print(preprocessed_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ENrOledq1j8"
      },
      "source": [
        "4. **Run the Code**\n",
        "\n",
        "Execute the script or notebook to see the output of the 'preprocessing steps'. The preprocessed_tokens variable should contain the cleaned and processed tokens from the sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUpab2Hrq94G"
      },
      "source": [
        "5. **Implement Bag of Words (BoW)**\n",
        "\n",
        "Add the following code to convert the preprocessed text into a Bag of Words model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwV7ch_5rGD8",
        "outputId": "ec9a92b5-7bb7-4d79-ecde-35421eb2c90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Names: ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (BoW):\n",
            "[[0 0 0 0 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 0 1 0 0 1 0 1 0 0 1 0]\n",
            " [1 0 1 1 0 0 0 1 0 1 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Hello world! This is an example of text preprocessing.\",\n",
        "    \"Text mining is an important aspect of data analysis.\",\n",
        "    \"Let's clean and prepare text data for analysis.\"\n",
        "]\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "document_matrix = X.toarray()\n",
        "\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"Document Matrix (BoW):\")\n",
        "print(document_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI7rU_V0rK8g"
      },
      "source": [
        "**6. Implement TF-IDF**\n",
        "\n",
        "Add the following code to convert the text into a TF-IDF representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kR0SfdlrI5V",
        "outputId": "1d2f825a-da7e-4bc9-da01-864cddc2007c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature Names (TF-IDF): ['analysis' 'aspect' 'clean' 'data' 'example' 'hello' 'important' 'let'\n",
            " 'mining' 'prepare' 'preprocessing' 'text' 'world']\n",
            "Document Matrix (TF-IDF):\n",
            "[[0.         0.         0.         0.         0.47952794 0.47952794\n",
            "  0.         0.         0.         0.         0.47952794 0.28321692\n",
            "  0.47952794]\n",
            " [0.35829137 0.4711101  0.         0.35829137 0.         0.\n",
            "  0.4711101  0.         0.4711101  0.         0.         0.27824521\n",
            "  0.        ]\n",
            " [0.35829137 0.         0.4711101  0.35829137 0.         0.\n",
            "  0.         0.4711101  0.         0.4711101  0.         0.27824521\n",
            "  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the documents\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get feature names and converted document matrix\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "document_matrix_tfidf = X_tfidf.toarray()\n",
        "\n",
        "print(\"Feature Names (TF-IDF):\", feature_names_tfidf)\n",
        "print(\"Document Matrix (TF-IDF):\")\n",
        "print(document_matrix_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWQ96dS8rUeI"
      },
      "source": [
        "***Exercises***\n",
        "\n",
        "* Implement Lemmatization: Replace the stemming process with lemmatization using WordNetLemmatizer.\n",
        "* Handle Different Languages: Modify the code to preprocess text in a different language using the appropriate stopwords and tokenizers.\n",
        "\n",
        "* Explore Other Vectorizers: Try using HashingVectorizer and compare it with CountVectorizer and TfidfVectorizer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
